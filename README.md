# ğŸ¦™ LLaMA Interactive Web App with OpenWebUI

> ğŸš€ **Deploy your own AI-powered chat interface in minutes!** This project provides a complete Docker setup for running LLaMA models with OpenWebUI on DigitalOcean droplets.

## ğŸŒŸ Features

- ğŸ¤– **LLaMA Models Integration** - Run powerful language models locally
- ğŸ¨ **Beautiful Web Interface** - Clean and intuitive OpenWebUI design
- ğŸ³ **Docker Containerized** - Easy deployment and management
- â˜ï¸ **Cloud Ready** - Optimized for DigitalOcean droplets
- ğŸ”’ **Secure Configuration** - Environment-based secrets management
- âš¡ **Quick Setup** - Get running in under 5 minutes

## ğŸ“‹ Prerequisites

Before you begin, ensure you have:

- ğŸ–¥ï¸ A DigitalOcean droplet (minimum 4GB RAM recommended)
- ğŸ³ Docker and Docker Compose installed
- ğŸ”‘ Git installed on your system
- ğŸŒ Basic knowledge of terminal/command line

## ğŸš€ Quick Start Guide

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/Cristina-CI/gruppoc.git
cd gruppoc
```

### 2ï¸âƒ£ Environment Configuration

Create your environment file for secure configuration:

```bash
cp .env.example .env
nano .env
```

Configure your `.env` file with the following variables:

```env
# OpenWebUI Configuration
WEBUI_SECRET_KEY=your-super-secret-key-here

# LLaMA Model Configuration
OLLAMA_BASE_URL=http://ollama:11434
DEFAULT_MODELS=llama2,codellama

# Security Settings
WEBUI_AUTH=true
ENABLE_SIGNUP=false

# Optional: Custom Settings
WEBUI_NAME="My AI Assistant"
DEFAULT_USER_ROLE=user
```

### 3ï¸âƒ£ Launch the Application

Start all services with Docker Compose:

```bash
docker-compose up -d
```

Wait for the containers to initialize (this may take a few minutes on first run):

```bash
docker-compose logs -f
```

### 4ï¸âƒ£ Access Your Web App

Once everything is running, access your application at:

```
https://gruppoc.cristina-capozziiorio.me/
```

## ğŸ› ï¸ Advanced Configuration

### ğŸ“ Project Structure

```
â”œâ”€â”€ docker-compose.yml          # Main Docker configuration
â”œâ”€â”€ .env.example               # Environment template
â”œâ”€â”€ .env                       # Your environment variables (create this)
â”œâ”€â”€ data/                      # Persistent data storage
â”‚   â”œâ”€â”€ ollama/               # LLaMA models and data
â”‚   â””â”€â”€ open-webui/           # OpenWebUI data and configs
â”œâ”€â”€ config/                    # Additional configurations
â””â”€â”€ README.md                 # This file
```

### ğŸ”§ Custom Model Installation

To add custom LLaMA models:

```bash
# Access the Ollama container
docker exec -it ollama-container ollama pull llama2:13b

# Or pull specific model variants
docker exec -it ollama-container ollama pull codellama:python
```

### ğŸ”’ Security Best Practices

1. **Change default secrets** in your `.env` file
2. **Use strong passwords** for admin accounts
3. **Configure firewall** rules on your droplet
4. **Enable HTTPS** with reverse proxy (nginx/traefik)
5. **Regular backups** of your data directory

## ğŸ› Troubleshooting

### Common Issues

**ğŸš¨ Port already in use**
```bash
# Check what's using the port
sudo lsof -i :3000
# Stop conflicting services
sudo systemctl stop apache2  # or nginx
```

**ğŸš¨ Container fails to start**
```bash
# Check container logs
docker-compose logs ollama
docker-compose logs open-webui
```

**ğŸš¨ Out of memory errors**
```bash
# Check system resources
free -h
df -h
# Consider upgrading your droplet size
```

### ğŸ“Š Monitoring

Monitor your application health:

```bash
# Check container status
docker-compose ps

# View resource usage
docker stats

# Check logs
docker-compose logs --tail=50
```

## ğŸ”„ Updates and Maintenance

### Updating the Application

```bash
# Pull latest changes
git pull origin main

# Rebuild containers
docker-compose down
docker-compose up -d --build

# Clean up old images
docker system prune -f
```

## ğŸ“ˆ Performance Optimization

- ğŸš€ Use SSD storage for better I/O performance
- ğŸ’¾ Allocate sufficient RAM (8GB+ recommended for larger models)
- ğŸŒ Use CDN for static assets in production
- âš¡ Enable GPU acceleration if available

## ğŸ¤ Contributing

We welcome contributions! Here's how you can help:

1. ğŸ´ Fork the repository
2. ğŸŒ¿ Create a feature branch (`git checkout -b feature/amazing-feature`)
3. ğŸ’¾ Commit your changes (`git commit -m 'Add some amazing feature'`)
4. ğŸ“¤ Push to the branch (`git push origin feature/amazing-feature`)
5. ğŸ”„ Open a Pull Request

## ğŸ‘¥ Contributors


- ğŸ‘¨â€ğŸ’» **Cristina-CI**
- ğŸ‘©â€ğŸ’» **Skitarrata**
- ğŸ‘¨â€ğŸ’» **gianmarcogg03**
- ğŸ‘¨â€ğŸ’» **RaffDeco**
- ğŸ‘¨â€ğŸ’» **SimoApo**



## ğŸ‰ Acknowledgments

- ğŸ¦™ [Ollama](https://ollama.ai/) for the amazing LLaMA integration
- ğŸ¨ [OpenWebUI](https://openwebui.com/) for the beautiful interface
- ğŸ³ [Docker](https://docker.com/) for containerization
- â˜ï¸ [DigitalOcean](https://digitalocean.com/) for reliable hosting

---

<div align="center">

**â­ If this project helped you, please give it a star! â­**

Made with â¤ï¸ and lots of â˜•

</div>
